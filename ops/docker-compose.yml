# AngelClaw – Local Development Compose
#
# Single-VPS setup for running ANGELNODE + Cloud backend + optional LLM.
# Run from the ops/ directory:
#
#   cd ops
#   docker-compose up --build
#
# ANGELNODE is exposed on 127.0.0.1:8400 (host loopback only).
# Cloud API is on 0.0.0.0:8500 (accessible from remote clients).
# Auth is enabled by default — see ops/config/angelclaw.env.
# Ollama is internal-only (no host port) — enable via LLM_ENABLED=true.
#
# To enable LLM:
#   1. Set LLM_ENABLED=true below
#   2. docker-compose up --build -d
#   3. docker-compose exec ollama ollama pull llama3
#   4. Test: curl -X POST http://127.0.0.1:8500/api/v1/llm/chat \
#        -H "Content-Type: application/json" -d '{"prompt":"hello"}'

services:
  angelnode:
    build:
      context: ..
      dockerfile: ops/docker/Dockerfile.angelnode
    ports:
      # SECURITY: bind to loopback so only the local host can reach the agent
      - "127.0.0.1:8400:8400"
    volumes:
      - angelnode-logs:/var/log/angelgrid
      # Mount config files so edits take effect without rebuilding
      - ../angelnode/config/default_policy.json:/app/angelnode/config/default_policy.json:ro
      - ../angelnode/config/category_defaults.json:/app/angelnode/config/category_defaults.json:ro
    environment:
      - ANGELNODE_POLICY_FILE=/app/angelnode/config/default_policy.json
      - ANGELNODE_CATEGORY_DEFAULTS_FILE=/app/angelnode/config/category_defaults.json
      - ANGELNODE_LOG_FILE=/var/log/angelgrid/decisions.jsonl
      - ANGELNODE_AGENT_ID=dev-agent-01
      # Cloud sync — register on startup and poll for policy updates
      - ANGELGRID_CLOUD_URL=http://cloud:8500
      - ANGELGRID_TENANT_ID=dev-tenant
      - ANGELGRID_SYNC_INTERVAL=60
    restart: unless-stopped
    depends_on:
      - cloud

  cloud:
    build:
      context: ..
      dockerfile: ops/docker/Dockerfile.cloud
    ports:
      # Bind to all interfaces so remote clients (Windows ANGELNODE) can connect.
      # Auth is required — credentials are in ops/config/angelclaw.env.
      - "0.0.0.0:8500:8500"
    expose:
      - "8500"
    volumes:
      - cloud-data:/data
    env_file:
      - config/angelclaw.env
    environment:
      - ANGELGRID_DATABASE_URL=sqlite:////data/angelgrid.db
      # LLM Proxy configuration
      # Set LLM_ENABLED=true to activate the /api/v1/llm/chat endpoint
      - LLM_ENABLED=false
      - LLM_BACKEND_URL=http://ollama:11434
      - LLM_MODEL=llama3
      - LLM_MAX_TOKENS=1024
      - LLM_TIMEOUT_SECONDS=60
    restart: unless-stopped
    depends_on:
      - ollama

  # --------------------------------------------------------------------------
  # Ollama – Local LLM inference (internal-only)
  #
  # SECURITY: Ollama has NO host port — only reachable from the Docker
  # network at http://ollama:11434.  Never expose to the internet.
  #
  # After first start, pull a model:
  #   docker-compose exec ollama ollama pull llama3
  # --------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    expose:
      - "11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    # Uncomment for GPU passthrough (NVIDIA):
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  angelnode-logs:
  cloud-data:
  ollama-data:
