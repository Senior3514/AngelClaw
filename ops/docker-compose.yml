# ANGELGRID – Local Development Compose
#
# Single-VPS setup for running ANGELNODE + Cloud backend + optional LLM.
# Run from the ops/ directory:
#
#   cd ops
#   docker compose up --build
#
# ANGELNODE is exposed on 127.0.0.1:8400 (host loopback only).
# Cloud API is internal-only (reachable at http://cloud:8500 from
# other containers, but not exposed to the host by default).
# Ollama is internal-only (no host port) — enable via LLM_ENABLED=true.

services:
  angelnode:
    build:
      context: ..
      dockerfile: ops/docker/Dockerfile.angelnode
    ports:
      # SECURITY: bind to loopback so only the local host can reach the agent
      - "127.0.0.1:8400:8400"
    volumes:
      - angelnode-logs:/var/log/angelgrid
      # Mount config files so edits take effect without rebuilding
      - ../angelnode/config/default_policy.json:/app/angelnode/config/default_policy.json:ro
      - ../angelnode/config/category_defaults.json:/app/angelnode/config/category_defaults.json:ro
    environment:
      - ANGELNODE_POLICY_FILE=/app/angelnode/config/default_policy.json
      - ANGELNODE_CATEGORY_DEFAULTS_FILE=/app/angelnode/config/category_defaults.json
      - ANGELNODE_LOG_FILE=/var/log/angelgrid/decisions.jsonl
      - ANGELNODE_AGENT_ID=dev-agent-01
      # Cloud sync — register on startup and poll for policy updates
      - ANGELGRID_CLOUD_URL=http://cloud:8500
      - ANGELGRID_TENANT_ID=dev-tenant
      - ANGELGRID_SYNC_INTERVAL=60
    restart: unless-stopped
    depends_on:
      - cloud

  cloud:
    build:
      context: ..
      dockerfile: ops/docker/Dockerfile.cloud
    # No host port mapping — cloud API is internal only.
    # Other containers reach it at http://cloud:8500.
    # Uncomment the next two lines to expose on the host for debugging:
    ports:
      - "127.0.0.1:8500:8500"
    expose:
      - "8500"
    volumes:
      - cloud-data:/data
    environment:
      - ANGELGRID_DATABASE_URL=sqlite:////data/angelgrid.db
      # LLM Proxy — disabled by default.  Set to "true" to enable.
      - LLM_ENABLED=false
      - LLM_BACKEND_URL=http://ollama:11434
      - LLM_MODEL=llama3
      - LLM_MAX_TOKENS=1024
      - LLM_TIMEOUT_SECONDS=30
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Ollama – Local LLM inference (optional, internal-only)
  #
  # To enable:
  #   1. Set LLM_ENABLED=true on the 'cloud' service above
  #   2. Uncomment the 'ollama' service below
  #   3. Run: docker compose up --build
  #   4. Pull a model: docker compose exec ollama ollama pull llama3
  #
  # SECURITY: Ollama has NO host port — only reachable from the Docker
  # network at http://ollama:11434.  Never expose to the internet.
  # --------------------------------------------------------------------------
  # ollama:
  #   image: ollama/ollama:latest
  #   expose:
  #     - "11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped
  #   # Uncomment for GPU passthrough (NVIDIA):
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]

volumes:
  angelnode-logs:
  cloud-data:
  # ollama-data:
