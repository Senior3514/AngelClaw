"""ANGELGRID Cloud – Guardian Chat Orchestrator.

Dual-mode chat:
  - Deterministic mode (LLM_ENABLED=false, default): regex-based intent
    detection with template responses in guardian angel tone.
  - LLM-backed mode (LLM_ENABLED=true): same intent detection to gather
    context, then context + prompt sent to LLM proxy. Falls back to
    deterministic mode if LLM is unreachable.

Action suggestions are ALWAYS deterministic (never generated by LLM).
"""

from __future__ import annotations

import logging
import re
from datetime import datetime, timedelta, timezone

import httpx
from sqlalchemy.orm import Session

from cloud.ai_assistant.assistant import propose_policy_tightening, summarize_recent_incidents
from cloud.api.guardian_models import ActionSuggestion, ChatRequest, ChatResponse
from cloud.db.models import (
    AgentNodeRow,
    EventRow,
    GuardianAlertRow,
    GuardianChangeRow,
    GuardianReportRow,
)
from cloud.services.predictive import predict_threat_vectors
from cloud.services.timeline import build_agent_timeline
from shared.security.secret_scanner import redact_secrets

logger = logging.getLogger("angelgrid.cloud.guardian_chat")

# ---------------------------------------------------------------------------
# Intent detection (regex-based)
# ---------------------------------------------------------------------------

_INTENT_PATTERNS: list[tuple[str, re.Pattern]] = [
    ("threats", re.compile(r"(?i)(threat|predict|risk|vector|landscape|danger)")),
    ("alerts", re.compile(r"(?i)(guardian.*alert|critical.*notif|warning|alarm)")),
    ("explain", re.compile(r"(?i)(explain|why.*(block|alert|allow)|what.happen|tell.*about.*event)")),
    ("propose", re.compile(r"(?i)(propose|suggest|recommend|tighten|improve.*polic|fix.*polic)")),
    ("agent_status", re.compile(r"(?i)(agent|fleet|node|status|health|online|offline)")),
    ("changes", re.compile(r"(?i)(change|what.*change|recent.*update|modif|policy.*update)")),
    ("incidents", re.compile(r"(?i)(incident|breach|attack|event|blocked|critical|high.sev)")),
    ("about", re.compile(r"(?i)(who.*are.*you|what.*are.*you|about|introduce|guardian)")),
    ("help", re.compile(r"(?i)(help|what.*can.*you|how.*do|command|feature)")),
]


def detect_intent(prompt: str) -> str:
    """Detect the user's intent from the prompt."""
    for intent, pattern in _INTENT_PATTERNS:
        if pattern.search(prompt):
            return intent
    return "general"


# ---------------------------------------------------------------------------
# Deterministic handlers
# ---------------------------------------------------------------------------

def _handle_incidents(db: Session, tenant_id: str) -> tuple[str, list[ActionSuggestion], list[str]]:
    summary = summarize_recent_incidents(db, tenant_id, lookback_hours=24)
    lines = [
        f"Here's your security summary for the last 24 hours:\n",
        f"Total incidents: **{summary.total_incidents}**",
    ]
    if summary.by_severity:
        sev_parts = [f"{s.severity}: {s.count}" for s in summary.by_severity]
        lines.append(f"By severity: {', '.join(sev_parts)}")
    if summary.by_classification:
        cls_parts = [f"{c.classification}: {c.count}" for c in summary.by_classification]
        lines.append(f"By type: {', '.join(cls_parts)}")
    if summary.top_affected_agents:
        agent_parts = [f"{a.hostname} ({a.incident_count})" for a in summary.top_affected_agents[:3]]
        lines.append(f"Most active agents: {', '.join(agent_parts)}")
    if summary.recommended_focus:
        lines.append("\nRecommendations:")
        for r in summary.recommended_focus:
            lines.append(f"  - {r}")

    actions = []
    if summary.total_incidents > 0:
        actions.append(ActionSuggestion(
            action_type="review_incidents",
            title="Review incidents",
            description="Open the incidents feed to see details",
            metadata={"endpoint": "/api/v1/incidents/recent"},
        ))
    refs = ["/api/v1/assistant/incidents"]
    return "\n".join(lines), actions, refs


def _handle_agent_status(db: Session) -> tuple[str, list[ActionSuggestion], list[str]]:
    agents = db.query(AgentNodeRow).all()
    if not agents:
        return "No agents registered yet. Start an ANGELNODE to see it here.", [], []

    active = [a for a in agents if a.status == "active"]
    degraded = [a for a in agents if a.status == "degraded"]
    offline = [a for a in agents if a.status not in ("active", "degraded")]

    lines = [
        f"Fleet overview: **{len(agents)}** agents total\n",
        f"  Active: {len(active)}",
        f"  Degraded: {len(degraded)}",
        f"  Offline/Other: {len(offline)}",
    ]
    if degraded:
        lines.append(f"\nDegraded agents: {', '.join(a.hostname for a in degraded)}")
    if offline:
        lines.append(f"Offline agents: {', '.join(a.hostname for a in offline)}")

    actions = []
    if offline:
        actions.append(ActionSuggestion(
            action_type="review_agent",
            title="Check offline agents",
            description=f"{len(offline)} agent(s) need attention",
        ))
    return "\n".join(lines), actions, ["/api/v1/agents"]


def _handle_threats(db: Session) -> tuple[str, list[ActionSuggestion], list[str]]:
    predictions = predict_threat_vectors(db, lookback_hours=24)
    if not predictions:
        return (
            "No threat vectors detected in the last 24 hours. "
            "Your systems look healthy — ANGELGRID is watching quietly in the background.",
            [], ["/api/v1/analytics/threat-matrix"],
        )

    lines = ["Predicted threat vectors (last 24h):\n"]
    for p in predictions:
        confidence_pct = int(p.confidence * 100)
        lines.append(f"  **{p.vector_name}** — {confidence_pct}% confidence")
        lines.append(f"    {p.rationale}")

    actions = [ActionSuggestion(
        action_type="review_threats",
        title="View threat matrix",
        description="Open the full threat landscape view",
        metadata={"endpoint": "/api/v1/analytics/threat-matrix"},
    )]
    return "\n".join(lines), actions, ["/api/v1/analytics/threat-matrix"]


def _handle_alerts(db: Session, tenant_id: str) -> tuple[str, list[ActionSuggestion], list[str]]:
    alerts = (
        db.query(GuardianAlertRow)
        .filter(GuardianAlertRow.tenant_id == tenant_id)
        .order_by(GuardianAlertRow.created_at.desc())
        .limit(5)
        .all()
    )
    if not alerts:
        return (
            "No guardian alerts right now. I'm watching for critical patterns "
            "like secret exfiltration attempts, severity spikes, and agent flapping.",
            [], [],
        )

    lines = [f"Recent guardian alerts ({len(alerts)}):\n"]
    for a in alerts:
        lines.append(f"  [{a.severity.upper()}] **{a.alert_type}**: {a.title}")
    return "\n".join(lines), [], ["/api/v1/guardian/alerts/recent"]


def _handle_changes(db: Session, tenant_id: str) -> tuple[str, list[ActionSuggestion], list[str]]:
    cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
    changes = (
        db.query(GuardianChangeRow)
        .filter(
            GuardianChangeRow.tenant_id == tenant_id,
            GuardianChangeRow.created_at >= cutoff,
        )
        .order_by(GuardianChangeRow.created_at.desc())
        .limit(10)
        .all()
    )
    if not changes:
        return "No policy or configuration changes in the last 24 hours.", [], []

    lines = [f"Recent changes ({len(changes)}):\n"]
    for c in changes:
        lines.append(f"  [{c.change_type}] {c.description} — by {c.changed_by}")
    return "\n".join(lines), [], ["/api/v1/guardian/changes"]


def _handle_propose(db: Session, tenant_id: str) -> tuple[str, list[ActionSuggestion], list[str]]:
    # Use first agent group or "all"
    proposals = propose_policy_tightening(db, "all", lookback_hours=24)
    lines = [proposals.analysis_summary]
    if proposals.proposed_rules:
        lines.append("\nProposed rules:")
        for r in proposals.proposed_rules:
            lines.append(f"  - **{r.description}** → {r.action} ({r.risk_level})")
            lines.append(f"    {r.rationale}")

    actions = []
    if proposals.proposed_rules:
        actions.append(ActionSuggestion(
            action_type="propose_rule",
            title="Review proposals",
            description=f"{len(proposals.proposed_rules)} rule(s) proposed",
            metadata={"endpoint": "/api/v1/assistant/propose"},
        ))
    return "\n".join(lines), actions, ["/api/v1/assistant/propose"]


def _handle_about() -> tuple[str, list[ActionSuggestion], list[str]]:
    return (
        "I'm your ANGELGRID Guardian Angel — an autonomous security companion "
        "that watches over your AI agents, servers, and infrastructure.\n\n"
        "I protect quietly in the background, like a seatbelt — not a speed bump. "
        "I only speak up when something genuinely dangerous happens: secret access, "
        "destructive commands, critical file modifications, or risky external calls.\n\n"
        "Everything else — analysis, reading, summarizing, reasoning — flows freely. "
        "I'm here to help you use AI safely, not to slow you down.",
        [], [],
    )


def _handle_help() -> tuple[str, list[ActionSuggestion], list[str]]:
    return (
        "Here's what you can ask me:\n\n"
        "  **Incidents** — \"What happened recently?\" / \"Show me incidents\"\n"
        "  **Agent status** — \"How's my fleet?\" / \"Which agents are offline?\"\n"
        "  **Threats** — \"Any threat predictions?\" / \"What risks do you see?\"\n"
        "  **Alerts** — \"Any guardian alerts?\" / \"Critical notifications?\"\n"
        "  **Changes** — \"What changed recently?\" / \"Policy updates?\"\n"
        "  **Proposals** — \"Suggest policy improvements\" / \"Tighten security\"\n"
        "  **Explain** — \"Why was this event blocked?\"\n"
        "  **About** — \"Who are you?\" / \"What do you do?\"\n\n"
        "I'm always watching in the background. Ask me anything about your security posture!",
        [], [],
    )


def _handle_general(db: Session, tenant_id: str) -> tuple[str, list[ActionSuggestion], list[str]]:
    """Fallback: return a brief status overview."""
    agents = db.query(AgentNodeRow).count()
    cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
    events_24h = db.query(EventRow).filter(EventRow.timestamp >= cutoff).count()
    reports = (
        db.query(GuardianReportRow)
        .filter(GuardianReportRow.tenant_id == tenant_id)
        .order_by(GuardianReportRow.timestamp.desc())
        .first()
    )

    lines = [
        "Here's a quick overview:\n",
        f"  Agents: {agents}",
        f"  Events (24h): {events_24h}",
    ]
    if reports:
        lines.append(f"  Last report: {reports.summary}")

    lines.append(
        "\nAsk me about incidents, threats, agent status, or policy proposals "
        "for more details. I'm your guardian angel — always here to help!"
    )
    return "\n".join(lines), [], []


# ---------------------------------------------------------------------------
# Main chat handler
# ---------------------------------------------------------------------------

async def handle_chat(db: Session, req: ChatRequest) -> ChatResponse:
    """Process a chat request. Deterministic by default, LLM-backed if enabled."""
    intent = detect_intent(req.prompt)
    logger.info("Guardian Chat — intent=%s, prompt_len=%d", intent, len(req.prompt))

    # Gather deterministic response
    answer, actions, refs = _dispatch_intent(db, intent, req.tenant_id)

    # Try LLM enrichment if enabled
    llm_answer = await _try_llm_enrichment(req.prompt, answer, intent)
    if llm_answer:
        answer = llm_answer

    # Redact any secrets from the answer
    safe_answer = redact_secrets(answer)

    return ChatResponse(
        answer=safe_answer,
        actions=actions,
        references=refs,
        intent=intent,
    )


def _dispatch_intent(
    db: Session, intent: str, tenant_id: str,
) -> tuple[str, list[ActionSuggestion], list[str]]:
    """Route to the appropriate deterministic handler."""
    if intent == "incidents":
        return _handle_incidents(db, tenant_id)
    elif intent == "agent_status":
        return _handle_agent_status(db)
    elif intent == "threats":
        return _handle_threats(db)
    elif intent == "alerts":
        return _handle_alerts(db, tenant_id)
    elif intent == "changes":
        return _handle_changes(db, tenant_id)
    elif intent == "propose":
        return _handle_propose(db, tenant_id)
    elif intent == "about":
        return _handle_about()
    elif intent == "help":
        return _handle_help()
    elif intent == "explain":
        return (
            "To explain a specific event, I need an event ID. "
            "You can find event IDs in the alerts feed or incidents list, "
            "then ask me: \"Explain event <id>\".\n\n"
            "Or try asking about recent incidents to see what's been happening.",
            [ActionSuggestion(
                action_type="review_incidents",
                title="View recent events",
                description="See the alerts feed for event IDs",
                metadata={"endpoint": "/api/v1/incidents/recent"},
            )],
            ["/api/v1/assistant/explain"],
        )
    else:
        return _handle_general(db, tenant_id)


async def _try_llm_enrichment(prompt: str, context_answer: str, intent: str) -> str | None:
    """Try to enrich the answer via the LLM proxy. Returns None if unavailable."""
    try:
        from cloud.llm_proxy.config import LLM_ENABLED
        if not LLM_ENABLED:
            return None
    except ImportError:
        return None

    try:
        enriched_prompt = (
            f"You are the ANGELGRID Guardian Angel. The user asked: \"{prompt}\"\n\n"
            f"Here is the factual data from the system (intent: {intent}):\n"
            f"{context_answer}\n\n"
            "Please provide a friendly, concise response incorporating this data. "
            "Keep the guardian angel tone — helpful, warm, protective. "
            "Never suggest broad restrictions. Focus on targeted, actionable advice."
        )
        async with httpx.AsyncClient(timeout=15) as client:
            resp = await client.post(
                "http://127.0.0.1:8500/api/v1/llm/chat",
                json={"prompt": enriched_prompt},
            )
            if resp.status_code == 200:
                data = resp.json()
                return data.get("answer")
    except Exception:
        logger.debug("LLM enrichment unavailable, using deterministic response")

    return None
